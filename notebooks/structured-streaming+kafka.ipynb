{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Streaming + Kafka 集成指南"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从卡夫卡读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建用于流查询的Kafka源\n",
    "\n",
    "```python\n",
    "# Subscribe to 1 topic\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Subscribe to 1 topic, with headers\n",
    "val df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"headers\")\n",
    "\n",
    "# Subscribe to multiple topics\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1,topic2\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Subscribe to a pattern\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribePattern\", \"topic.*\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "```\n",
    "\n",
    "### 创建用于批查询的Kafka源\n",
    "\n",
    "若用例更适合批处理，可以在定义的偏移范围创建一个DataSet/DataFrame。\n",
    "\n",
    "```python\n",
    "# Subscribe to 1 topic defaults to the earliest and latest offsets\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Subscribe to multiple topics, specifying explicit Kafka offsets\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1,topic2\") \\\n",
    "  .option(\"startingOffsets\", \"\"\"{\"topic1\":{\"0\":23,\"1\":-2},\"topic2\":{\"0\":-2}}\"\"\") \\\n",
    "  .option(\"endingOffsets\", \"\"\"{\"topic1\":{\"0\":50,\"1\":-1},\"topic2\":{\"0\":-1}}\"\"\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Subscribe to a pattern, at the earliest and latest offsets\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribePattern\", \"topic.*\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "```\n",
    "\n",
    "源的每行有以下数据结构：\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Column</th><th>Type</th></tr>\n",
    "<tr>\n",
    "  <td>key</td>\n",
    "  <td>binary</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>value</td>\n",
    "  <td>binary</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>topic</td>\n",
    "  <td>string</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>partition</td>\n",
    "  <td>int</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>offset</td>\n",
    "  <td>long</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>timestamp</td>\n",
    "  <td>timestamp</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>timestampType</td>\n",
    "  <td>int</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>headers (optional)</td>\n",
    "  <td>array</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "对于批查询和流查询，必须为Kafka源设置以下选项：\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Option</th><th>value</th><th>meaning</th></tr>\n",
    "<tr>\n",
    "  <td>assign</td>\n",
    "  <td>json string {\"topicA\":[0,1],\"topicB\":[2,4]}</td>\n",
    "  <td>Specific TopicPartitions to consume.\n",
    "  Only one of \"assign\", \"subscribe\" or \"subscribePattern\"\n",
    "  options can be specified for Kafka source.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>subscribe</td>\n",
    "  <td>A comma-separated list of topics</td>\n",
    "  <td>The topic list to subscribe.\n",
    "  Only one of \"assign\", \"subscribe\" or \"subscribePattern\"\n",
    "  options can be specified for Kafka source.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>subscribePattern</td>\n",
    "  <td>Java regex string</td>\n",
    "  <td>The pattern used to subscribe to topic(s).\n",
    "  Only one of \"assign, \"subscribe\" or \"subscribePattern\"\n",
    "  options can be specified for Kafka source.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>kafka.bootstrap.servers</td>\n",
    "  <td>A comma-separated list of host:port</td>\n",
    "  <td>The Kafka \"bootstrap.servers\" configuration.</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "下列配置是可选的：\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Option</th><th>value</th><th>default</th><th>query type</th><th>meaning</th></tr>\n",
    "<tr>\n",
    "  <td>startingOffsetsByTimestamp</td>\n",
    "  <td>json string\n",
    "  \"\"\" {\"topicA\":{\"0\": 1000, \"1\": 1000}, \"topicB\": {\"0\": 2000, \"1\": 2000}} \"\"\"\n",
    "  </td>\n",
    "  <td>none (the value of <code>startingOffsets</code> will apply)</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>The start point of timestamp when a query is started, a json string specifying a starting timestamp for\n",
    "  each TopicPartition. The returned offset for each partition is the earliest offset whose timestamp is greater than or\n",
    "  equal to the given timestamp in the corresponding partition. If the matched offset doesn't exist,\n",
    "  the query will fail immediately to prevent unintended read from such partition. (This is a kind of limitation as of now, and will be addressed in near future.)<p />\n",
    "  <p />\n",
    "  Spark simply passes the timestamp information to <code>KafkaConsumer.offsetsForTimes</code>, and doesn't interpret or reason about the value. <p />\n",
    "  For more details on <code>KafkaConsumer.offsetsForTimes</code>, please refer <a href=\"https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#offsetsForTimes-java.util.Map-\">javadoc</a> for details.<p />\n",
    "  Also the meaning of <code>timestamp</code> here can be vary according to Kafka configuration (<code>log.message.timestamp.type</code>): please refer <a href=\"https://kafka.apache.org/documentation/\">Kafka documentation</a> for further details.<p />\n",
    "  Note: This option requires Kafka 0.10.1.0 or higher.<p />\n",
    "  Note2: <code>startingOffsetsByTimestamp</code> takes precedence over <code>startingOffsets</code>.<p />\n",
    "  Note3: For streaming queries, this only applies when a new query is started, and that resuming will\n",
    "  always pick up from where the query left off. Newly discovered partitions during a query will start at\n",
    "  earliest.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>startingOffsets</td>\n",
    "  <td>\"earliest\", \"latest\" (streaming only), or json string\n",
    "  \"\"\" {\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-2}} \"\"\"\n",
    "  </td>\n",
    "  <td>\"latest\" for streaming, \"earliest\" for batch</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>The start point when a query is started, either \"earliest\" which is from the earliest offsets,\n",
    "  \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for\n",
    "  each TopicPartition.  In the json, -2 as an offset can be used to refer to earliest, -1 to latest.\n",
    "  Note: For batch queries, latest (either implicitly or by using -1 in json) is not allowed.\n",
    "  For streaming queries, this only applies when a new query is started, and that resuming will\n",
    "  always pick up from where the query left off. Newly discovered partitions during a query will start at\n",
    "  earliest.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>endingOffsetsByTimestamp</td>\n",
    "  <td>json string\n",
    "  \"\"\" {\"topicA\":{\"0\": 1000, \"1\": 1000}, \"topicB\": {\"0\": 2000, \"1\": 2000}} \"\"\"\n",
    "  </td>\n",
    "  <td>latest</td>\n",
    "  <td>batch query</td>\n",
    "  <td>The end point when a batch query is ended, a json string specifying an ending timestamp for each TopicPartition.\n",
    "  The returned offset for each partition is the earliest offset whose timestamp is greater than or equal to\n",
    "  the given timestamp in the corresponding partition. If the matched offset doesn't exist, the offset will\n",
    "  be set to latest.<p />\n",
    "  <p />\n",
    "  Spark simply passes the timestamp information to <code>KafkaConsumer.offsetsForTimes</code>, and doesn't interpret or reason about the value. <p />\n",
    "  For more details on <code>KafkaConsumer.offsetsForTimes</code>, please refer <a href=\"https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#offsetsForTimes-java.util.Map-\">javadoc</a> for details.<p />\n",
    "  Also the meaning of <code>timestamp</code> here can be vary according to Kafka configuration (<code>log.message.timestamp.type</code>): please refer <a href=\"https://kafka.apache.org/documentation/\">Kafka documentation</a> for further details.<p />\n",
    "  Note: This option requires Kafka 0.10.1.0 or higher.<p />\n",
    "  Note2: <code>endingOffsetsByTimestamp</code> takes precedence over <code>endingOffsets</code>.\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>endingOffsets</td>\n",
    "  <td>latest or json string\n",
    "  {\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-1}}\n",
    "  </td>\n",
    "  <td>latest</td>\n",
    "  <td>batch query</td>\n",
    "  <td>The end point when a batch query is ended, either \"latest\" which is just referred to the\n",
    "  latest, or a json string specifying an ending offset for each TopicPartition.  In the json, -1\n",
    "  as an offset can be used to refer to latest, and -2 (earliest) as an offset is not allowed.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>failOnDataLoss</td>\n",
    "  <td>true or false</td>\n",
    "  <td>true</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or\n",
    "  offsets are out of range). This may be a false alarm. You can disable it when it doesn't work\n",
    "  as you expected.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>kafkaConsumer.pollTimeoutMs</td>\n",
    "  <td>long</td>\n",
    "  <td>512</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>The timeout in milliseconds to poll data from Kafka in executors.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>fetchOffset.numRetries</td>\n",
    "  <td>int</td>\n",
    "  <td>3</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>Number of times to retry before giving up fetching Kafka offsets.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>fetchOffset.retryIntervalMs</td>\n",
    "  <td>long</td>\n",
    "  <td>10</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>milliseconds to wait before retrying to fetch Kafka offsets</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>maxOffsetsPerTrigger</td>\n",
    "  <td>long</td>\n",
    "  <td>none</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>Rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>minPartitions</td>\n",
    "  <td>int</td>\n",
    "  <td>none</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>Desired minimum number of partitions to read from Kafka.\n",
    "  By default, Spark has a 1-1 mapping of topicPartitions to Spark partitions consuming from Kafka.\n",
    "  If you set this option to a value greater than your topicPartitions, Spark will divvy up large\n",
    "  Kafka partitions to smaller pieces. Please note that this configuration is like a <code>hint</code>: the\n",
    "  number of Spark tasks will be <strong>approximately</strong> <code>minPartitions</code>. It can be less or more depending on\n",
    "  rounding errors or Kafka partitions that didn't receive any new data.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>groupIdPrefix</td>\n",
    "  <td>string</td>\n",
    "  <td>spark-kafka-source</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>Prefix of consumer group identifiers (<code>group.id</code>) that are generated by structured streaming\n",
    "  queries. If \"kafka.group.id\" is set, this option will be ignored.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>kafka.group.id</td>\n",
    "  <td>string</td>\n",
    "  <td>none</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>The Kafka group id to use in Kafka consumer while reading from Kafka. Use this with caution.\n",
    "  By default, each query generates a unique group id for reading data. This ensures that each Kafka\n",
    "  source has its own consumer group that does not face interference from any other consumer, and\n",
    "  therefore can read all of the partitions of its subscribed topics. In some scenarios (for example,\n",
    "  Kafka group-based authorization), you may want to use a specific authorized group id to read data.\n",
    "  You can optionally set the group id. However, do this with extreme caution as it can cause\n",
    "  unexpected behavior. Concurrently running queries (both, batch and streaming) or sources with the\n",
    "  same group id are likely interfere with each other causing each query to read only part of the\n",
    "  data. This may also occur when queries are started/restarted in quick succession. To minimize such\n",
    "  issues, set the Kafka consumer session timeout (by setting option \"kafka.session.timeout.ms\") to\n",
    "  be very small. When this is set, option \"groupIdPrefix\" will be ignored.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>includeHeaders</td>\n",
    "  <td>boolean</td>\n",
    "  <td>false</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>Whether to include the Kafka headers in the row.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
