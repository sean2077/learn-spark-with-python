{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Streaming + Kafka 集成指南"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从Kafka读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建用于流查询的Kafka源\n",
    "\n",
    "```python\n",
    "# Subscribe to 1 topic\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Subscribe to 1 topic, with headers\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"headers\")\n",
    "\n",
    "# Subscribe to multiple topics\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1,topic2\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Subscribe to a pattern\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribePattern\", \"topic.*\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "```\n",
    "\n",
    "### 创建用于批查询的Kafka源\n",
    "\n",
    "若用例更适合批处理，可以在定义的偏移范围创建一个DataSet/DataFrame。\n",
    "\n",
    "```python\n",
    "# Subscribe to 1 topic defaults to the earliest and latest offsets\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Subscribe to multiple topics, specifying explicit Kafka offsets\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1,topic2\") \\\n",
    "  .option(\"startingOffsets\", \"\"\"{\"topic1\":{\"0\":23,\"1\":-2},\"topic2\":{\"0\":-2}}\"\"\") \\\n",
    "  .option(\"endingOffsets\", \"\"\"{\"topic1\":{\"0\":50,\"1\":-1},\"topic2\":{\"0\":-1}}\"\"\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Subscribe to a pattern, at the earliest and latest offsets\n",
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribePattern\", \"topic.*\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "```\n",
    "\n",
    "源的每行有以下数据结构：\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Column</th><th>Type</th></tr>\n",
    "<tr>\n",
    "  <td>key</td>\n",
    "  <td>binary</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>value</td>\n",
    "  <td>binary</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>topic</td>\n",
    "  <td>string</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>partition</td>\n",
    "  <td>int</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>offset</td>\n",
    "  <td>long</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>timestamp</td>\n",
    "  <td>timestamp</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>timestampType</td>\n",
    "  <td>int</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>headers (optional)</td>\n",
    "  <td>array</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "对于批查询和流查询，必须为Kafka源设置以下选项：\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Option</th><th>value</th><th>meaning</th></tr>\n",
    "<tr>\n",
    "  <td>assign</td>\n",
    "  <td>json string {\"topicA\":[0,1],\"topicB\":[2,4]}</td>\n",
    "  <td>指定要消费的TopicPartitions. 仅可指定\"assign\"，\"subscribe\"，\"subscribePattern\"选项之一。\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>subscribe</td>\n",
    "  <td>逗号分隔的topics列表</td>\n",
    "  <td>要订阅的topic列表。仅可指定\"assign\"，\"subscribe\"，\"subscribePattern\"选项之一。\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>subscribePattern</td>\n",
    "  <td>Java正则string</td>\n",
    "  <td>要订阅的topics的pattern。仅可指定\"assign\"，\"subscribe\"，\"subscribePattern\"选项之一。\n",
    "  </td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>kafka.bootstrap.servers</td>\n",
    "  <td>逗号分隔的`host:port`列表</td>\n",
    "  <td>Kafka \"bootstrap.servers\" 配置.</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "下列配置是可选的：\n",
    "\n",
    "<table class=\"table\">\n",
    "  <tr>\n",
    "    <th>Option</th>\n",
    "    <th>value</th>\n",
    "    <th>default</th>\n",
    "    <th>query type</th>\n",
    "    <th>meaning</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>startingOffsetsByTimestamp</td>\n",
    "    <td>\n",
    "      json string \"\"\" {\"topicA\":{\"0\": 1000, \"1\": 1000}, \"topicB\": {\"0\": 2000,\n",
    "      \"1\": 2000}} \"\"\"\n",
    "    </td>\n",
    "    <td>none (将应用<code>startingOffsets</code>的值)</td>\n",
    "    <td>streaming and batch</td>\n",
    "    <td>\n",
    "      启动查询时的时间戳记的起始点，一个json字符串，为每个TopicPartition指定一个起始时间戳记。每个分区返回的偏移量是最早的偏移量，其时间戳大于或等于相应分区中的给定时间戳记。\n",
    "      如果不存在匹配的偏移量，查询将立即失败，以防止意外读取该分区。（到目前为止，这是一种限制，并将在不久的将来解决。）\n",
    "      <p />\n",
    "      <p />\n",
    "      Spark simply passes the timestamp information to\n",
    "      <code>KafkaConsumer.offsetsForTimes</code>, and doesn't interpret or\n",
    "      reason about the value.\n",
    "      <p />\n",
    "      For more details on <code>KafkaConsumer.offsetsForTimes</code>, please\n",
    "      refer\n",
    "      <a\n",
    "        href=\"https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#offsetsForTimes-java.util.Map-\"\n",
    "        >javadoc</a\n",
    "      >\n",
    "      for details.\n",
    "      <p />\n",
    "      Also the meaning of <code>timestamp</code> here can be vary according to\n",
    "      Kafka configuration (<code>log.message.timestamp.type</code>): please\n",
    "      refer\n",
    "      <a href=\"https://kafka.apache.org/documentation/\">Kafka documentation</a>\n",
    "      for further details.\n",
    "      <p />\n",
    "      Note: This option requires Kafka 0.10.1.0 or higher.\n",
    "      <p />\n",
    "      Note2: <code>startingOffsetsByTimestamp</code> takes precedence over\n",
    "      <code>startingOffsets</code>.\n",
    "      <p />\n",
    "      Note3: For streaming queries, this only applies when a new query is\n",
    "      started, and that resuming will always pick up from where the query left\n",
    "      off. Newly discovered partitions during a query will start at earliest.\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>startingOffsets</td>\n",
    "    <td>\n",
    "      \"earliest\", \"latest\" (streaming only), or json string \"\"\"\n",
    "      {\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-2}} \"\"\"\n",
    "    </td>\n",
    "    <td>\"latest\" for streaming, \"earliest\" for batch</td>\n",
    "    <td>streaming and batch</td>\n",
    "    <td>\n",
    "      The start point when a query is started, either \"earliest\" which is from\n",
    "      the earliest offsets, \"latest\" which is just from the latest offsets, or a\n",
    "      json string specifying a starting offset for each TopicPartition. In the\n",
    "      json, -2 as an offset can be used to refer to earliest, -1 to latest.\n",
    "      Note: For batch queries, latest (either implicitly or by using -1 in json)\n",
    "      is not allowed. For streaming queries, this only applies when a new query\n",
    "      is started, and that resuming will always pick up from where the query\n",
    "      left off. Newly discovered partitions during a query will start at\n",
    "      earliest.\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>endingOffsetsByTimestamp</td>\n",
    "    <td>\n",
    "      json string \"\"\" {\"topicA\":{\"0\": 1000, \"1\": 1000}, \"topicB\": {\"0\": 2000,\n",
    "      \"1\": 2000}} \"\"\"\n",
    "    </td>\n",
    "    <td>latest</td>\n",
    "    <td>batch query</td>\n",
    "    <td>\n",
    "      The end point when a batch query is ended, a json string specifying an\n",
    "      ending timestamp for each TopicPartition. The returned offset for each\n",
    "      partition is the earliest offset whose timestamp is greater than or equal\n",
    "      to the given timestamp in the corresponding partition. If the matched\n",
    "      offset doesn't exist, the offset will be set to latest.\n",
    "      <p />\n",
    "      <p />\n",
    "      Spark simply passes the timestamp information to\n",
    "      <code>KafkaConsumer.offsetsForTimes</code>, and doesn't interpret or\n",
    "      reason about the value.\n",
    "      <p />\n",
    "      For more details on <code>KafkaConsumer.offsetsForTimes</code>, please\n",
    "      refer\n",
    "      <a\n",
    "        href=\"https://kafka.apache.org/21/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#offsetsForTimes-java.util.Map-\"\n",
    "        >javadoc</a\n",
    "      >\n",
    "      for details.\n",
    "      <p />\n",
    "      Also the meaning of <code>timestamp</code> here can be vary according to\n",
    "      Kafka configuration (<code>log.message.timestamp.type</code>): please\n",
    "      refer\n",
    "      <a href=\"https://kafka.apache.org/documentation/\">Kafka documentation</a>\n",
    "      for further details.\n",
    "      <p />\n",
    "      Note: This option requires Kafka 0.10.1.0 or higher.\n",
    "      <p />\n",
    "      Note2: <code>endingOffsetsByTimestamp</code> takes precedence over\n",
    "      <code>endingOffsets</code>.\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>endingOffsets</td>\n",
    "    <td>latest or json string {\"topicA\":{\"0\":23,\"1\":-1},\"topicB\":{\"0\":-1}}</td>\n",
    "    <td>latest</td>\n",
    "    <td>batch query</td>\n",
    "    <td>\n",
    "      The end point when a batch query is ended, either \"latest\" which is just\n",
    "      referred to the latest, or a json string specifying an ending offset for\n",
    "      each TopicPartition. In the json, -1 as an offset can be used to refer to\n",
    "      latest, and -2 (earliest) as an offset is not allowed.\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>failOnDataLoss</td>\n",
    "    <td>true or false</td>\n",
    "    <td>true</td>\n",
    "    <td>streaming and batch</td>\n",
    "    <td>\n",
    "      Whether to fail the query when it's possible that data is lost (e.g.,\n",
    "      topics are deleted, or offsets are out of range). This may be a false\n",
    "      alarm. You can disable it when it doesn't work as you expected.\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>kafkaConsumer.pollTimeoutMs</td>\n",
    "    <td>long</td>\n",
    "    <td>512</td>\n",
    "    <td>streaming and batch</td>\n",
    "    <td>The timeout in milliseconds to poll data from Kafka in executors.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>fetchOffset.numRetries</td>\n",
    "    <td>int</td>\n",
    "    <td>3</td>\n",
    "    <td>streaming and batch</td>\n",
    "    <td>Number of times to retry before giving up fetching Kafka offsets.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>fetchOffset.retryIntervalMs</td>\n",
    "    <td>long</td>\n",
    "    <td>10</td>\n",
    "    <td>streaming and batch</td>\n",
    "    <td>milliseconds to wait before retrying to fetch Kafka offsets</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>maxOffsetsPerTrigger</td>\n",
    "    <td>long</td>\n",
    "    <td>none</td>\n",
    "    <td>streaming and batch</td>\n",
    "    <td>\n",
    "      Rate limit on maximum number of offsets processed per trigger interval.\n",
    "      The specified total number of offsets will be proportionally split across\n",
    "      topicPartitions of different volume.\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>minPartitions</td>\n",
    "    <td>int</td>\n",
    "    <td>none</td>\n",
    "    <td>streaming and batch</td>\n",
    "    <td>\n",
    "      Desired minimum number of partitions to read from Kafka. By default, Spark\n",
    "      has a 1-1 mapping of topicPartitions to Spark partitions consuming from\n",
    "      Kafka. If you set this option to a value greater than your\n",
    "      topicPartitions, Spark will divvy up large Kafka partitions to smaller\n",
    "      pieces. Please note that this configuration is like a <code>hint</code>:\n",
    "      the number of Spark tasks will be <strong>approximately</strong>\n",
    "      <code>minPartitions</code>. It can be less or more depending on rounding\n",
    "      errors or Kafka partitions that didn't receive any new data.\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>groupIdPrefix</td>\n",
    "    <td>string</td>\n",
    "    <td>spark-kafka-source</td>\n",
    "    <td>streaming and batch</td>\n",
    "    <td>\n",
    "      Prefix of consumer group identifiers (<code>group.id</code>) that are\n",
    "      generated by structured streaming queries. If \"kafka.group.id\" is set,\n",
    "      this option will be ignored.\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>kafka.group.id</td>\n",
    "    <td>string</td>\n",
    "    <td>none</td>\n",
    "    <td>streaming and batch</td>\n",
    "    <td>\n",
    "      The Kafka group id to use in Kafka consumer while reading from Kafka. Use\n",
    "      this with caution. By default, each query generates a unique group id for\n",
    "      reading data. This ensures that each Kafka source has its own consumer\n",
    "      group that does not face interference from any other consumer, and\n",
    "      therefore can read all of the partitions of its subscribed topics. In some\n",
    "      scenarios (for example, Kafka group-based authorization), you may want to\n",
    "      use a specific authorized group id to read data. You can optionally set\n",
    "      the group id. However, do this with extreme caution as it can cause\n",
    "      unexpected behavior. Concurrently running queries (both, batch and\n",
    "      streaming) or sources with the same group id are likely interfere with\n",
    "      each other causing each query to read only part of the data. This may also\n",
    "      occur when queries are started/restarted in quick succession. To minimize\n",
    "      such issues, set the Kafka consumer session timeout (by setting option\n",
    "      \"kafka.session.timeout.ms\") to be very small. When this is set, option\n",
    "      \"groupIdPrefix\" will be ignored.\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>includeHeaders</td>\n",
    "    <td>boolean</td>\n",
    "    <td>false</td>\n",
    "    <td>streaming and batch</td>\n",
    "    <td>Whether to include the Kafka headers in the row.</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 消费者缓存\n",
    "\n",
    "初始化Kafka消费者很耗时，尤其是在处理时间是关键因素的流方案中。因此，Spark通过利用Apache Commons Pool在执行者上汇集了Kafka消费者。\n",
    "\n",
    "缓存密钥是根据以下信息构建的：\n",
    "- Topic name\n",
    "- Topic partition\n",
    "- Group ID\n",
    "\n",
    "以下属性可用于配置消费者池：\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr>\n",
    "<tr>\n",
    "  <td>spark.kafka.consumer.cache.capacity</td>\n",
    "  <td>64</td>\n",
    "  <td>The maximum number of consumers cached. Please note that it's a soft limit.</td>\n",
    "  <td>3.0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>spark.kafka.consumer.cache.timeout</td>\n",
    "  <td>5m (5 minutes)</td>\n",
    "  <td>The minimum amount of time a consumer may sit idle in the pool before it is eligible for eviction by the evictor.</td>\n",
    "  <td>3.0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>spark.kafka.consumer.cache.evictorThreadRunInterval</td>\n",
    "  <td>1m (1 minute)</td>\n",
    "  <td>The interval of time between runs of the idle evictor thread for consumer pool. When non-positive, no idle evictor thread will be run.</td>\n",
    "  <td>3.0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>spark.kafka.consumer.cache.jmx.enable</td>\n",
    "  <td>false</td>\n",
    "  <td>Enable or disable JMX for pools created with this configuration instance. Statistics of the pool are available via JMX instance.\n",
    "  The prefix of JMX name is set to \"kafka010-cached-simple-kafka-consumer-pool\".\n",
    "  </td>\n",
    "  <td>3.0.0</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "池的大小受限制spark.kafka.consumer.cache.capacity，但它用作“软限制”以不阻止Spark任务。\n",
    "\n",
    "空闲驱逐线程会定期删除使用时间不超过给定超时的使用者。如果借用时达到此阈值，它将尝试删除当前未使用的最少使用的条目。\n",
    "\n",
    "如果无法将其删除，则池将保持增长。在最坏的情况下，池将增长到可以在执行程序中运行的最大并发任务数（即任务插槽数）。\n",
    "\n",
    "如果任务由于任何原因失败，出于安全原因，将使用新创建的Kafka使用者执行新任务。同时，我们使池中具有相同缓存密钥的所有使用者失效，以删除执行失败时使用的使用者。正在使用其他任务的使用者将不会关闭，但是当他们返回到池中时，它们也会失效。\n",
    "\n",
    "与消费者一起，Spark会分别合并从Kafka获取的记录，以使Kafka消费者在Spark的观点上保持无国籍状态，并最大程度地提高合并效率。它利用与Kafka使用者池相同的缓存密钥。请注意，由于特性差异，它没有利用Apache Commons Pool。\n",
    "\n",
    "以下属性可用于配置获取的数据池：\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr>\n",
    "<tr>\n",
    "  <td>spark.kafka.consumer.fetchedData.cache.timeout</td>\n",
    "  <td>5m (5 minutes)</td>\n",
    "  <td>The minimum amount of time a fetched data may sit idle in the pool before it is eligible for eviction by the evictor.</td>\n",
    "  <td>3.0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>spark.kafka.consumer.fetchedData.cache.evictorThreadRunInterval</td>\n",
    "  <td>1m (1 minute)</td>\n",
    "  <td>The interval of time between runs of the idle evictor thread for fetched data pool. When non-positive, no idle evictor thread will be run.</td>\n",
    "  <td>3.0.0</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将数据写入Kafka\n",
    "\n",
    "在这里，我们描述了向Apache Kafka编写流查询和批处理查询的支持。请注意，Apache Kafka仅支持至少一次写入语义。因此，在向Kafka写入流式查询或批处理查询时，某些记录可能会重复。例如，如果Kafka需要重试经纪人未确认的消息（即使该经纪人接收并写入了消息记录），就会发生这种情况。由于这些Kafka写语义，结构化流无法阻止此类重复发生。但是，如果编写查询成功，则可以假定查询输出至少编写了一次。在读取写入的数据时删除重复项的可能解决方案可能是引入主键（唯一），该键可用于在读取时执行重复数据删除。\n",
    "\n",
    "写入Kafka的Dataframe在架构中应包含以下几列：\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Column</th><th>Type</th></tr>\n",
    "<tr>\n",
    "  <td>key (optional)</td>\n",
    "  <td>string or binary</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>value (required)</td>\n",
    "  <td>string or binary</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>headers (optional)</td>\n",
    "  <td>array</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>topic (*optional)</td>\n",
    "  <td>string</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>partition (optional)</td>\n",
    "  <td>int</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "*如果未指定“ topic”配置选项，则topic列为必填项。\n",
    "\n",
    "值列是唯一必需的选项。如果未指定null键列，则将自动添加有价键列（请参阅有关如何null处理有价键值的Kafka语义）。如果存在主题列，则在将给定行写入Kafka时将其值用作主题，除非设置了“ topic”配置选项，即，“ topic”配置选项会覆盖主题列。如果未指定“分区”列（或其值为null），则由Kafka生产者计算分区。可以通过设置kafka.partitioner.class选项在Spark中指定Kafka分区程序 。如果不存在，将使用Kafka默认分区程序。\n",
    "\n",
    "对于批量查询和流查询，必须为Kafka接收器设置以下选项。\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Option</th><th>value</th><th>meaning</th></tr>\n",
    "<tr>\n",
    "  <td>kafka.bootstrap.servers</td>\n",
    "  <td>A comma-separated list of host:port</td>\n",
    "  <td>The Kafka \"bootstrap.servers\" configuration.</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "以下配置是可选的：\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Option</th><th>value</th><th>default</th><th>query type</th><th>meaning</th></tr>\n",
    "<tr>\n",
    "  <td>topic</td>\n",
    "  <td>string</td>\n",
    "  <td>none</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>Sets the topic that all rows will be written to in Kafka. This option overrides any\n",
    "  topic column that may exist in the data.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>includeHeaders</td>\n",
    "  <td>boolean</td>\n",
    "  <td>false</td>\n",
    "  <td>streaming and batch</td>\n",
    "  <td>Whether to include the Kafka headers in the row.</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建用于流式查询的Kafka接收器\n",
    "\n",
    "```python\n",
    "# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\n",
    "ds = df \\\n",
    "  .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"topic\", \"topic1\") \\\n",
    "  .start()\n",
    "\n",
    "# Write key-value data from a DataFrame to Kafka using a topic specified in the data\n",
    "ds = df \\\n",
    "  .selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .writeStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .start()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将批查询的输出写入Kafka\n",
    "\n",
    "```python\n",
    "# Write key-value data from a DataFrame to a specific Kafka topic specified in an option\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .write \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"topic\", \"topic1\") \\\n",
    "  .save()\n",
    "\n",
    "# Write key-value data from a DataFrame to Kafka using a topic specified in the data\n",
    "df.selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "  .write \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生产者缓存\n",
    "\n",
    "给定Kafka生产者实例被设计为线程安全的，Spark初始化一个Kafka生产者实例，并在任务中共同使用同一缓存密钥。\n",
    "\n",
    "缓存密钥是根据以下信息构建的：\n",
    "\n",
    "- Kafka生产者配置\n",
    "\n",
    "这包括授权配置，当使用委派令牌时，Spark将自动包括该配置。即使我们考虑了授权，也可以期望在相同的Kafka生产者配置中使用相同的Kafka生产者实例。委托令牌更新时，它将使用不同的Kafka生产者；旧的授权令牌的Kafka生产者实例将根据缓存策略被驱逐。\n",
    "\n",
    "以下属性可用于配置生产者池：\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr>\n",
    "<tr>\n",
    "  <td>spark.kafka.producer.cache.timeout</td>\n",
    "  <td>10m (10 minutes)</td>\n",
    "  <td>The minimum amount of time a producer may sit idle in the pool before it is eligible for eviction by the evictor.</td>\n",
    "  <td>2.2.1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>spark.kafka.producer.cache.evictorThreadRunInterval</td>\n",
    "  <td>1m (1 minute)</td>\n",
    "  <td>The interval of time between runs of the idle evictor thread for producer pool. When non-positive, no idle evictor thread will be run.</td>\n",
    "  <td>3.0.0</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "空闲驱逐线程会定期删除使用时间不超过给定超时的生产者。请注意，生产者是共享和并发使用的，因此最后一次使用的时间戳由返回生产者实例且引用计数为0的时刻确定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka特定配置\n",
    "\n",
    "Kafka自己的配置可以通过设置DataStreamReader.option与kafka.前缀，例如 stream.option(\"kafka.bootstrap.servers\", \"host:port\")。有关可能的kafka参数，请参阅 Kafka使用者配置文档以获取与读取数据相关的参数，以及Kafka生产者配置文档 以获取与写入数据相关的参数。\n",
    "\n",
    "请注意，无法设置以下Kafka参数，并且Kafka源或接收器将引发异常：\n",
    "\n",
    "- **group.id**：Kafka源将自动为每个查询创建一个唯一的组ID。用户可以通过可选的source选项设置自动生成的group.id的前缀groupIdPrefix，默认值为“ spark-kafka-source”。您也可以设置“ kafka.group.id”以强制Spark使用特殊的组ID，但是，请阅读此选项的警告并谨慎使用。\n",
    "- **auto.offset.reset**：设置源选项startingOffsets以指定从何处开始。结构化流管理在内部管理哪些偏移量，而不是依靠kafka使用者来执行此操作。这将确保在动态订阅新主题/分区时不会丢失任何数据。请注意，startingOffsets仅在启动新的流查询时适用，并且恢复将始终从查询中断的地方开始。\n",
    "- **key.deserializer**：始终使用ByteArrayDeserializer将键反序列化为字节数组。使用DataFrame操作显式反序列化键。\n",
    "- **value.deserializer**：始终使用ByteArrayDeserializer将值反序列化为字节数组。使用DataFrame操作显式反序列化值。\n",
    "- **key.serializer**：密钥始终使用ByteArraySerializer或StringSerializer进行序列化。使用DataFrame操作可以将密钥显式序列化为字符串或字节数组。\n",
    "- **value.serializer**：值始终使用ByteArraySerializer或StringSerializer进行序列化。使用DataFrame操作可以将值显式序列化为字符串或字节数组。\n",
    "- **enable.auto.commit**：Kafka源不提交任何偏移量。\n",
    "- **Interceptor.classes**：Kafka源始终将键和值读取为字节数组。使用ConsumerInterceptor是不安全的，因为它可能会中断查询。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 部署\n",
    "\n",
    "与任何Spark应用程序一样，spark-submit用于启动您的应用程序。spark-sql-kafka-0-10_2.12 其依赖项可以直接添加到spark-submit使用中--packages，例如，\n",
    "\n",
    "```bash\n",
    "./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1 ...\n",
    "```\n",
    "\n",
    "为了进行实验spark-shell，您还可以直接使用--packagesaddspark-sql-kafka-0-10_2.12及其依赖项\n",
    "\n",
    "```bash\n",
    "./bin/spark-shell --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"security\">Security</h2>\n",
    "\n",
    "<p>Kafka 0.9.0.0 introduced several features that increases security in a cluster. For detailed\n",
    "description about these possibilities, see <a href=\"http://kafka.apache.org/documentation.html#security\">Kafka security docs</a>.</p>\n",
    "\n",
    "<p>It&#8217;s worth noting that security is optional and turned off by default.</p>\n",
    "\n",
    "<p>Spark supports the following ways to authenticate against Kafka cluster:</p>\n",
    "<ul>\n",
    "  <li><strong>Delegation token (introduced in Kafka broker 1.1.0)</strong></li>\n",
    "  <li><strong>JAAS login configuration</strong></li>\n",
    "</ul>\n",
    "\n",
    "<h3 id=\"delegation-token\">Delegation token</h3>\n",
    "\n",
    "<p>This way the application can be configured via Spark parameters and may not need JAAS login\n",
    "configuration (Spark can use Kafka&#8217;s dynamic JAAS configuration feature). For further information\n",
    "about delegation tokens, see <a href=\"http://kafka.apache.org/documentation/#security_delegation_token\">Kafka delegation token docs</a>.</p>\n",
    "\n",
    "<p>The process is initiated by Spark&#8217;s Kafka delegation token provider. When <code class=\"highlighter-rouge\">spark.kafka.clusters.${cluster}.auth.bootstrap.servers</code> is set,\n",
    "Spark considers the following log in options, in order of preference:</p>\n",
    "<ul>\n",
    "  <li><strong>JAAS login configuration</strong>, please see example below.</li>\n",
    "  <li>\n",
    "    <p><strong>Keytab file</strong>, such as,</p>\n",
    "\n",
    "    <div class=\"highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>./bin/spark-submit \\\n",
    "    --keytab &lt;KEYTAB_FILE&gt; \\\n",
    "    --principal &lt;PRINCIPAL&gt; \\\n",
    "    --conf spark.kafka.clusters.${cluster}.auth.bootstrap.servers=&lt;KAFKA_SERVERS&gt; \\\n",
    "    ...\n",
    "</code></pre></div>    </div>\n",
    "  </li>\n",
    "  <li>\n",
    "    <p><strong>Kerberos credential cache</strong>, such as,</p>\n",
    "\n",
    "    <div class=\"highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>./bin/spark-submit \\\n",
    "    --conf spark.kafka.clusters.${cluster}.auth.bootstrap.servers=&lt;KAFKA_SERVERS&gt; \\\n",
    "    ...\n",
    "</code></pre></div>    </div>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<p>The Kafka delegation token provider can be turned off by setting <code class=\"highlighter-rouge\">spark.security.credentials.kafka.enabled</code> to <code class=\"highlighter-rouge\">false</code> (default: <code class=\"highlighter-rouge\">true</code>).</p>\n",
    "\n",
    "<p>Spark can be configured to use the following authentication protocols to obtain token (it must match with\n",
    "Kafka broker configuration):</p>\n",
    "<ul>\n",
    "  <li><strong>SASL SSL (default)</strong></li>\n",
    "  <li><strong>SSL</strong></li>\n",
    "  <li><strong>SASL PLAINTEXT (for testing)</strong></li>\n",
    "</ul>\n",
    "\n",
    "<p>After obtaining delegation token successfully, Spark distributes it across nodes and renews it accordingly.\n",
    "Delegation token uses <code class=\"highlighter-rouge\">SCRAM</code> login module for authentication and because of that the appropriate\n",
    "<code class=\"highlighter-rouge\">spark.kafka.clusters.${cluster}.sasl.token.mechanism</code> (default: <code class=\"highlighter-rouge\">SCRAM-SHA-512</code>) has to be configured. Also, this parameter\n",
    "must match with Kafka broker configuration.</p>\n",
    "\n",
    "<p>When delegation token is available on an executor Spark considers the following log in options, in order of preference:</p>\n",
    "<ul>\n",
    "  <li><strong>JAAS login configuration</strong>, please see example below.</li>\n",
    "  <li><strong>Delegation token</strong>, please see <code>spark.kafka.clusters.${cluster}.target.bootstrap.servers.regex</code> parameter for further details.</li>\n",
    "</ul>\n",
    "\n",
    "<p>When none of the above applies then unsecure connection assumed.</p>\n",
    "\n",
    "<h4 id=\"configuration\">Configuration</h4>\n",
    "\n",
    "<p>Delegation tokens can be obtained from multiple clusters and <code>${cluster}</code> is an arbitrary unique identifier which helps to group different configurations.</p>\n",
    "\n",
    "<table class=\"table\">\n",
    "<tr><th>Property Name</th><th>Default</th><th>Meaning</th><th>Since Version</th></tr>\n",
    "  <tr>\n",
    "    <td><code>spark.kafka.clusters.${cluster}.auth.bootstrap.servers</code></td>\n",
    "    <td>None</td>\n",
    "    <td>\n",
    "      A list of coma separated host/port pairs to use for establishing the initial connection\n",
    "      to the Kafka cluster. For further details please see Kafka documentation. Only used to obtain delegation token.\n",
    "    </td>\n",
    "    <td>3.0.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code>spark.kafka.clusters.${cluster}.target.bootstrap.servers.regex</code></td>\n",
    "    <td>.*</td>\n",
    "    <td>\n",
    "      Regular expression to match against the <code>bootstrap.servers</code> config for sources and sinks in the application.\n",
    "      If a server address matches this regex, the delegation token obtained from the respective bootstrap servers will be used when connecting.\n",
    "      If multiple clusters match the address, an exception will be thrown and the query won't be started.\n",
    "      Kafka's secure and unsecure listeners are bound to different ports. When both used the secure listener port has to be part of the regular expression.\n",
    "    </td>\n",
    "    <td>3.0.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code>spark.kafka.clusters.${cluster}.security.protocol</code></td>\n",
    "    <td>SASL_SSL</td>\n",
    "    <td>\n",
    "      Protocol used to communicate with brokers. For further details please see Kafka documentation. Protocol is applied on all the sources and sinks as default where\n",
    "      <code>bootstrap.servers</code> config matches (for further details please see <code>spark.kafka.clusters.${cluster}.target.bootstrap.servers.regex</code>),\n",
    "      and can be overridden by setting <code>kafka.security.protocol</code> on the source or sink.\n",
    "    </td>\n",
    "    <td>3.0.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code>spark.kafka.clusters.${cluster}.sasl.kerberos.service.name</code></td>\n",
    "    <td>kafka</td>\n",
    "    <td>\n",
    "      The Kerberos principal name that Kafka runs as. This can be defined either in Kafka's JAAS config or in Kafka's config.\n",
    "      For further details please see Kafka documentation. Only used to obtain delegation token.\n",
    "    </td>\n",
    "    <td>3.0.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code>spark.kafka.clusters.${cluster}.ssl.truststore.location</code></td>\n",
    "    <td>None</td>\n",
    "    <td>\n",
    "      The location of the trust store file. For further details please see Kafka documentation. Only used to obtain delegation token.\n",
    "    </td>\n",
    "    <td>3.0.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code>spark.kafka.clusters.${cluster}.ssl.truststore.password</code></td>\n",
    "    <td>None</td>\n",
    "    <td>\n",
    "      The store password for the trust store file. This is optional and only needed if <code>spark.kafka.clusters.${cluster}.ssl.truststore.location</code> is configured.\n",
    "      For further details please see Kafka documentation. Only used to obtain delegation token.\n",
    "    </td>\n",
    "    <td>3.0.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code>spark.kafka.clusters.${cluster}.ssl.keystore.location</code></td>\n",
    "    <td>None</td>\n",
    "    <td>\n",
    "      The location of the key store file. This is optional for client and can be used for two-way authentication for client.\n",
    "      For further details please see Kafka documentation. Only used to obtain delegation token.\n",
    "    </td>\n",
    "    <td>3.0.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code>spark.kafka.clusters.${cluster}.ssl.keystore.password</code></td>\n",
    "    <td>None</td>\n",
    "    <td>\n",
    "      The store password for the key store file. This is optional and only needed if <code>spark.kafka.clusters.${cluster}.ssl.keystore.location</code> is configured.\n",
    "      For further details please see Kafka documentation. Only used to obtain delegation token.\n",
    "    </td>\n",
    "    <td>3.0.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code>spark.kafka.clusters.${cluster}.ssl.key.password</code></td>\n",
    "    <td>None</td>\n",
    "    <td>\n",
    "      The password of the private key in the key store file. This is optional for client.\n",
    "      For further details please see Kafka documentation. Only used to obtain delegation token.\n",
    "    </td>\n",
    "    <td>3.0.0</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code>spark.kafka.clusters.${cluster}.sasl.token.mechanism</code></td>\n",
    "    <td>SCRAM-SHA-512</td>\n",
    "    <td>\n",
    "      SASL mechanism used for client connections with delegation token. Because SCRAM login module used for authentication a compatible mechanism has to be set here.\n",
    "      For further details please see Kafka documentation (<code>sasl.mechanism</code>). Only used to authenticate against Kafka broker with delegation token.\n",
    "    </td>\n",
    "    <td>3.0.0</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<h4 id=\"kafka-specific-configurations-1\">Kafka Specific Configurations</h4>\n",
    "\n",
    "<p>Kafka&#8217;s own configurations can be set with <code class=\"highlighter-rouge\">kafka.</code> prefix, e.g, <code class=\"highlighter-rouge\">--conf spark.kafka.clusters.${cluster}.kafka.retries=1</code>.\n",
    "For possible Kafka parameters, see <a href=\"http://kafka.apache.org/documentation.html#adminclientconfigs\">Kafka adminclient config docs</a>.</p>\n",
    "\n",
    "<h4 id=\"caveats\">Caveats</h4>\n",
    "\n",
    "<ul>\n",
    "  <li>Obtaining delegation token for proxy user is not yet supported (<a href=\"https://issues.apache.org/jira/browse/KAFKA-6945\">KAFKA-6945</a>).</li>\n",
    "</ul>\n",
    "\n",
    "<h3 id=\"jaas-login-configuration\">JAAS login configuration</h3>\n",
    "\n",
    "<p>JAAS login configuration must placed on all nodes where Spark tries to access Kafka cluster.\n",
    "This provides the possibility to apply any custom authentication logic with a higher cost to maintain.\n",
    "This can be done several ways. One possibility is to provide additional JVM parameters, such as,</p>\n",
    "\n",
    "<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>./bin/spark-submit \\\n",
    "    --driver-java-options \"-Djava.security.auth.login.config=/path/to/custom_jaas.conf\" \\\n",
    "    --conf spark.executor.extraJavaOptions=-Djava.security.auth.login.config=/path/to/custom_jaas.conf \\\n",
    "    ...\n",
    "</code></pre></div></div>\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "325px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
