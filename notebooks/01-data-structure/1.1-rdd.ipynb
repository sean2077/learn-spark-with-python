{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 弹性分布式数据集RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化Spark\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sparkContext=sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见RDD转换和动作见[常见转换](#常见转换)，[常见动作](#常见动作)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD的概念\n",
    "\n",
    "RDD(Resilient Distributed Datasets)，**弹性分布式数据集**，是spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。\n",
    "\n",
    "\n",
    "1. 分区（Partition），即数据集的基本组成单位。对于RDD来说，每个分区都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分区个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。（注：源代码中部分地方使用分片Slice，是为了保持向后兼容性）\n",
    "\n",
    "2. 计算每个分区的函数。Spark中RDD的计算是以分区为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。\n",
    "\n",
    "3. RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。\n",
    "\n",
    "4. Partitioner，即RDD的分区函数。当前Spark中实现了两种类型的分区函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分区数量，也决定了parent RDD Shuffle输出时的分区数量。\n",
    "\n",
    "5. 列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD创建方式\n",
    "\n",
    "创建RDD的方法有两种：\n",
    "\n",
    "- `并行化`驱动程序中的现有集合\n",
    "\n",
    "- `引用外部存储系统`（例如共享文件系统，HDFS，HBase或提供Hadoop InputFormat的任何数据源）中的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 并行化\n",
    "\n",
    "`SparkContext.parallelize`方法读取可迭代对象或集合(如:`dict`,`list`,`tuple`,`set`)，转换为可以并行计算的分布式数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from a list\n",
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog', 'fish']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from a tuple\n",
    "rdd = sc.parallelize(('cat', 'dog', 'fish'))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 'dog', 'fish'), ('orange', 'apple')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from a list of tuple\n",
    "list_t = [('cat', 'dog', 'fish'), ('orange', 'apple')]\n",
    "rdd = sc.parallelize(list_t)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'cat', 'fish']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from a set\n",
    "s = {'cat', 'dog', 'fish', 'cat', 'dog', 'dog'}\n",
    "rdd = sc.parallelize(s)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于`dict`, 仅用keys构建RDD\n",
    "d = {\n",
    "    'a': 100,\n",
    "    'b': 200,\n",
    "    'c': 300\n",
    "}\n",
    "rdd = sc.parallelize(d)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引用外部数据集\n",
    "\n",
    "PySpark可以从Hadoop支持的任何存储源创建分布式数据集，包括本地文件系统，HDFS，Cassandra，HBase，Amazon S3等。Spark支持文本文件，SequenceFiles和任何其他Hadoop InputFormat。\n",
    "\n",
    "`SparkContext.textFile`读取文件的URI（本地路径，或一个hdfs://，s3a://等URI），返回字符串RDD，其中一个字符串元素对应文件的一行。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',mpg,cyl,disp,hp,drat,wt,qsec,vs,am,gear,carb',\n",
       " 'Mazda RX4,21,6,160,110,3.9,2.62,16.46,0,1,4,4',\n",
       " 'Mazda RX4 Wag,21,6,160,110,3.9,2.875,17.02,0,1,4,4',\n",
       " 'Datsun 710,22.8,4,108,93,3.85,2.32,18.61,1,1,4,1',\n",
       " 'Hornet 4 Drive,21.4,6,258,110,3.08,3.215,19.44,1,0,3,1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取文件\n",
    "rdd = sc.textFile('../../data/mtcars.csv')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked',\n",
       " '1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S',\n",
       " '2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C',\n",
       " '3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S',\n",
       " '4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 支持输入目录\n",
    "rdd = sc.textFile('../../data/titanic/')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked',\n",
       " '1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S',\n",
       " '2,1,1,\"Cumings, Mrs. John Bradley (Florence Briggs Thayer)\",female,38,1,0,PC 17599,71.2833,C85,C',\n",
       " '3,1,3,\"Heikkinen, Miss. Laina\",female,26,0,0,STON/O2. 3101282,7.925,,S',\n",
       " '4,1,1,\"Futrelle, Mrs. Jacques Heath (Lily May Peel)\",female,35,1,0,113803,53.1,C123,S']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 支持输入文件通配符\n",
    "rdd = sc.textFile('../../data/*.csv')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除文本文件外，Spark的Python API还支持其他几种数据格式：\n",
    "\n",
    "- `SparkContext.wholeTextFiles`读取包含多个小文本文件的目录，并将每个小文本文件作为（文件名，内容）对返回。相比textFile，会在每个文件的每一行返回一条记录。\n",
    "\n",
    "- `RDD.saveAsPickleFile`和`SparkContext.pickleFile`支持将RDD保存为由pickle序列化的Python对象组成的简单格式。可批处理pickle序列化，默认批处理大小为10。\n",
    "\n",
    "- SequenceFile和Hadoop输入/输出格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('file:/mnt/Workspace/spark/learn-spark-with-python/data/twitter.txt',\n",
       " 'Fresh install of XP on new computer. Sweet relief! fuck vista\\t1018769417\\t1.0\\nWell. Now I know where to go when I want my knives. #ChiChevySXSW http://post.ly/RvDl\\t10284216536\\t1.0\\n\"Literally six weeks before I can take off \"\"SSC Chair\"\" off my email. Its like the torturous 4th mile before everything stops hurting.\"\\t10298589026\\t1.0\\nMitsubishi i MiEV - Wikipedia, the free encyclopedia - http://goo.gl/xipe Cutest car ever!\\t109017669432377344\\t1.0\\n\\'Cheap Eats in SLP\\' - http://t.co/4w8gRp7\\t109642968603963392\\t1.0\\nTeenage Mutant Ninja Turtle art is never a bad thing... http://bit.ly/aDMHyW\\t10995492579\\t1.0\\nNew demographic survey of online video viewers: http://bit.ly/cx8b7I via @KellyOlexa\\t11713360136\\t1.0\\nhi all - i\\'m going to be tweeting things lookstat at the @lookstat twitter account. please follow me there\\t1208319583\\t1.0\\nHoly carp, no. That movie will seriously suffer for it. RT @MouseInfo: Anyone excited for The Little Mermaid in 3D?\\t121330835726155776\\t1.0\\n\"Did I really need to learn \"\"I bought a box and put in it things\"\" in arabic? This is the most random book ever.\"\\t12358025545\\t1.0\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.wholeTextFiles(\"../../data\")\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD操作\n",
    "\n",
    "RDD支持两种类型的操作：`转换`（transformations，从现有数据集创建新数据集）和`动作`（actions，在对数据集执行计算后，将值返回给驱动程序）。\n",
    "\n",
    "Spark中的所有转换都是惰性的，因为它们不会立即计算出结果。相反，他们只记得应用于某些基本数据集（例如文件）的转换。仅当动作要求将结果返回给驱动程序时才计算转换。这种设计使Spark可以更高效地运行。例如，我们可以意识到，通过创建的数据集map将用于中，reduce并且仅将结果返回reduce给驱动程序，而不是将较大的映射数据集返回给驱动程序。\n",
    "\n",
    "默认情况下，每次在其上执行动作时，都可能会重新计算每个转换后的RDD。但也可使用persist（或cache）方法将RDD保留在内存中，在这种情况下，Spark会将元素保留在集群中，以便下次查询时可以更快地进行访问。还支持将RDD持久保存在磁盘上，或在多个节点之间复制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1087\n",
      "1087\n"
     ]
    }
   ],
   "source": [
    "### 使用lambda表达式\n",
    "# 统计字符数\n",
    "lines = sc.textFile(\"../../data/twitter.txt\")\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)\n",
    "\n",
    "# 将RDD保存在内存中\n",
    "lineLengths.persist()\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 使用函数\n",
    "# 统计字数\n",
    "def word_count(s):\n",
    "    words = s.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "wordCnts = lines.map(word_count)\n",
    "wordCnts.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 传类方法或成员\n",
    "# 注意：尽量不要传对象的成员或方法，因为如果引用某个对象的成员或方法，则整个对象都要传到集群中，如下\n",
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def func(self, s):\n",
    "        return s\n",
    "    def doStuff(self, rdd):\n",
    "        return rdd.map(self.func)\n",
    "    def doStuff2(self, rdd):\n",
    "        return rdd.map(lambda s: self.field + s)\n",
    "        \n",
    "# 为避免上述问题，可将需要引用的成员或方法复制为局部变量再传入\n",
    "class MyClass2(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def func(self, s):\n",
    "        return s\n",
    "    def doStuff(self, rdd):\n",
    "        _func = self.func\n",
    "        return rdd.map(_func)\n",
    "    def doStuff2(self, rdd):\n",
    "        _filed = self.field\n",
    "        return rdd.map(lambda s: _field + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"Did I really need to learn \"\"I bought a box and put in it things\"\" in arabic? This is the most random book ever.\"\\t12358025545\\t1.0',\n",
       "  1),\n",
       " ('\"Literally six weeks before I can take off \"\"SSC Chair\"\" off my email. Its like the torturous 4th mile before everything stops hurting.\"\\t10298589026\\t1.0',\n",
       "  1),\n",
       " (\"'Cheap Eats in SLP' - http://t.co/4w8gRp7\\t109642968603963392\\t1.0\", 1),\n",
       " ('Fresh install of XP on new computer. Sweet relief! fuck vista\\t1018769417\\t1.0',\n",
       "  1),\n",
       " ('Holy carp, no. That movie will seriously suffer for it. RT @MouseInfo: Anyone excited for The Little Mermaid in 3D?\\t121330835726155776\\t1.0',\n",
       "  1),\n",
       " ('Mitsubishi i MiEV - Wikipedia, the free encyclopedia - http://goo.gl/xipe Cutest car ever!\\t109017669432377344\\t1.0',\n",
       "  1),\n",
       " ('New demographic survey of online video viewers: http://bit.ly/cx8b7I via @KellyOlexa\\t11713360136\\t1.0',\n",
       "  1),\n",
       " ('Teenage Mutant Ninja Turtle art is never a bad thing... http://bit.ly/aDMHyW\\t10995492579\\t1.0',\n",
       "  1),\n",
       " ('Well. Now I know where to go when I want my knives. #ChiChevySXSW http://post.ly/RvDl\\t10284216536\\t1.0',\n",
       "  1),\n",
       " (\"hi all - i'm going to be tweeting things lookstat at the @lookstat twitter account. please follow me there\\t1208319583\\t1.0\",\n",
       "  1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 使用键值对\n",
    "# 统计字符数\n",
    "lines = sc.textFile(\"../../data/twitter.txt\")\n",
    "pairs = lines.map(lambda s: (s, 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "counts.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要传闭包函数，则必须要了解跨集群执行代码时的变量和方法的范围和生命周期。\n",
    "\n",
    "下述示例为计算RDD元素之和的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter value:  0\n"
     ]
    }
   ],
   "source": [
    "### 闭包\n",
    "counter = 0\n",
    "rdd = sc.parallelize([1,2,3,4])\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(\"Counter value: \", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark执行RDD操作会将任务分解到多个执行程序，而每个执行程序上的闭包的变量都是一个副本，因此在程序中的counter最终还是0。\n",
    "\n",
    "通常，闭包不应改变某些全局状态。Spark不定义或保证从闭包外部引用的对象的改变行为。某些执行此操作的代码可能会在本地模式下工作，但这只是偶然的情况，此类代码在分布式模式下将无法按预期运行。如果需要某些全局汇总，可使用累加器`Accumulator`。\n",
    "\n",
    "另外，关于打印RDD元素也要注意这个问题，在单台机器上使用`rdd.foreach(print)`或`rdd.map(print)`是可行的，但在集群模式下应该使用`collect`方法，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重组操作\n",
    "\n",
    "重组（Shuffle）是Spark用于重新分配数据的机制，以便在分区之间进行不同地分组。Spark中重新分区操作（如：`repartition`,`coalesce`）、'ByKey操作（如：`groupByKey`,`reduceByKey`）以及join操作（如：`cogroup`，`join`）会触发重组。重组通常涉及跨执行程序和机器复制数据，造成计算成本很高。\n",
    "\n",
    "重组操作可通过调整相关配置参数进行优化，具体见[Spark Cofiguration Guide](https://spark.apache.org/docs/latest/configuration.html#shuffle-behavior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常见操作\n",
    "\n",
    "常见RDD转换和动作间[常见转换](#常见转换)，[常见动作](#常见动作)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常见转换\n",
    "\n",
    "转换|含义\n",
    ":--|:--\n",
    "**map**(*func*)|返回一个新的RDD，该RDD是由将源数据集的每个元素经过函数*func*计算后返回值组成。\n",
    "**filter**(*func*) | 返回一个新的RDD，该RDD由经过*func*函数计算后返回值为true的输入元素组成\n",
    "**flatMap**(*func*) | 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以*func*应该返回一个序列，而不是单一元素）\n",
    "**mapPartitions**(*func*) | 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，*func*的函数类型必须是Iterator&lt;T> => Iterator&lt;U>\n",
    "**mapPartitionsWithIndex**(*func*) | 类似于mapPartitions，但*func*带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，*func*的函数类型必须是(Int, Interator&lt;T>) => Iterator&lt;U>\n",
    "**sample**(*withReplacement*, *fraction*, *seed*) | 根据*fraction*指定的比例对数据进行采样，可以选择是否使用随机数进行替换，*seed*用于指定随机数生成器种子\n",
    "**union**(*otherDataset*) | 对源RDD和参数RDD求并集后返回一个新的RDD\n",
    "**intersection**(*otherDataset*) | 对源RDD和参数RDD求交集后返回一个新的RDD\n",
    "**distinct**(\\[*numPartitions*\\])) | 对源RDD进行去重后返回一个新的RDD\n",
    "**groupByKey**(\\[*numPartitions*\\]) | 在(K,V)对的RDD上调用，返回(K, Iterator&lt;V>)对的RDD。<br>**注意:**(1)如果要分组以便对每个键执行汇总（例如求和或平均值），则使用`reduceByKey`或`aggregateByKey`性能会更好。<br>(2)默认情况下，输出中的并行度取决于父RDD的分区数。可以传递一个可选numPartitions参数来设置不同数量的任务。\n",
    "**reduceByKey**(*func*, \\[*numPartitions*\\]) | 在(K,V)对的RDD上调用，返回(K,V)对的RDD，其中每个键的值由指定的reduce函数*func*汇总得到，函数类型必须为(V,V) => V。与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置\n",
    "**aggregateByKey**(*zeroValue*)(*seqOp*, *combOp*, \\[*numPartitions*\\]) | 在(K,V)对的RDD上调用，返回(K,U)对的RDD，其中每个键的值由给定的组合函数*combOp*加中性\"零\"值进行汇总。允许输入值类型与汇合值类型不同，采用此方法可避免一些不必要的分配。例如：aggregateByKey(0)(_+_,_+_) 对k/y的RDD进行操作\n",
    "**sortByKey**(\\[*ascending*\\], \\[*numPartitions*\\]) | 在(K,V)对的RDD上调用，返回按K排序的(K,V)对RDD。必须实现Ordered接口。\n",
    "**sortBy**(*func*,\\[*ascending*\\], \\[*numPartitions*\\]) | 与sortByKey类似，但是更灵活。第一个参数是根据什么排序；第二个是否升序；第三个排序后分区数，默认与原RDD一样。\n",
    "**join**(*otherDataset*, \\[*numPartitions*\\]) | 在(K,V)和(K,W)的RDD上调用，返回(K,(V,W))的RDD，其中每个键都有所有成对的元素，相当于内连接(求交集)。外连接通过外连接leftOuterJoin，rightOuterJoin和fullOuterJoin支持。\n",
    "**leftOuterJoin** | leftOuterJoin类似于SQL中的左外关联left outer join，返回结果以前面的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可。\n",
    "**rightOuterJoin** | rightOuterJoin类似于SQL中的有外关联right outer join，返回结果以参数中的RDD为主，关联不上的记录为空。只能用于两个RDD之间的关联，如果要多个RDD关联，多关联几次即可\n",
    "**cogroup**(*otherDataset*, \\[*numPartitions*\\]) | 在(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable&lt;V>,Iterable<W&gt))类型的RDD。此操作也称`groupWith`\n",
    "**cartesian**(*otherDataset*) | 在类型T和U的RDD上调用时，返回（T，U）所有元素对（笛卡尔积）的RDD\n",
    "**pipe**(*command*, \\[*envVars*\\]) | 通过管道RDD的每个分区传递给Shell命令（例如Perl或bash脚本）。将RDD元素写入进程的stdin，并将输出到其stdout的行作为字符串的RDD返回。\n",
    "**coalesce**(*numPartitions*) | 重新分区，第一个参数是要分多少区，第二个参数是否shuffle，默认false，少分区变多分区-true，多分区变少分区-false\n",
    "**repartition**(*numPartitions*) | 重新分区，必须shuffle，参数是要分多少区\n",
    "**repartitionAndSortWithinPartitions**(*partitioner*) | 根据给定的分区程序*partitioner*对RDD重新分区，并在每个结果分区中，按其键对记录进行排序。这比`repartition`在每个分区内调用然后排序更为有效，因为它可以洗牌时就进行排序。\n",
    "**foldByKey**(*zeroValue*)(*seqOp*) | 该函数用于K/V做折叠，合并处理。与aggregate类似，第一个括号的参数应用于每个V值  第二括号函数是聚合例如：_+_\n",
    "**combineByKey** | 合并相同的key的值。rdd1.combineByKey(x => x, (a: Int, b: Int) => a + b, (m: Int, n: Int) => m + n)\n",
    "**partitionBy**(*partitioner*) | 对RDD进行分区。partitioner是分区器 例如new HashPartition(2\n",
    "**cache**(), **persist**() | RDD缓存，可以避免重复计算从而减少时间。区别：cache内部调用了persist算子，cache默认就一个缓存级别MEMORY-ONLY ，而persist则可以选择缓存级别\n",
    "**subtract**(*rdd*) | 返回前RDD元素不在后*rdd*的RDD\n",
    "**subtractByKey**(*otherRDD*) | substractByKey和subtract类似，只不过这里是针对K的，返回在主RDD中出现，并且不在*otherRDD*中出现的元素\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常见动作\n",
    "动作|含义\n",
    ":--|:--\n",
    "**reduce**(*func*) | 使用函数*func*(该函数接受两个参数并返回一个)来汇总数据集的元素。该函数应该是可交换的和关联的，以便可以并行正确地计算它。\n",
    "**collect**() | 在驱动程序中，以数组的形式返回数据集的所有元素\n",
    "**count**() | 返回RDD的元素个数\n",
    "**first**() | 返回RDD的第一个元素(类似于take(1))\n",
    "**take**(*n*) | 返回一个由数据集的前*n*个元素组成的数组\n",
    "**takeSample**(*withReplacement*,*num*, \\[*seed*\\]) | 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子\n",
    "**takeOrdered**(*n*, \\[*ordering*\\]) | 使用自然顺序或自定义比较器*ordering*返回RDD的前n个元素。\n",
    "**saveAsTextFile**(*path*) | 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的一行文本\n",
    "**saveAsSequenceFile**(*path*) (Java和Scala)| 在本地文件系统，HDFS或任何其他Hadoop支持的文件系统的给定路径中，将数据集的元素作为Hadoop SequenceFile写入。在实现Hadoop的Writable接口的键值对的RDD上可用。在Scala中，它也可用于隐式转换为Writable的类型(Spark包括对基本类型(如Int，Double，String等)的转换)。\n",
    "**saveAsObjectFile**(*path*) (Java和Scala)| 使用Java序列化以简单的格式编写数据集的元素，然后可以使用`SparkContext.objectFile()`进行加载。\n",
    "**countByKey**() | 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。\n",
    "**foreach**(*func*) | 在数据集的每一个元素上，运行函数*func*进行更新。\n",
    "**foreachPartition** | \n",
    "**aggregate** | 先对分区进行操作，在总体操作\n",
    "**reduceByKeyLocally** | \n",
    "**lookup** | \n",
    "**top** | \n",
    "**fold** | \n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD持久化\n",
    "\n",
    "RDD允许用户在执行多个查询时通过`persist()`或`cache()`方法显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提高了查询速度。Spark缓存具有容错性，若某个分区RDD丢失，则次分区会自动重新计算。\n",
    "\n",
    "RDD持久化可通过设定StorageLevel参数来以不同的方式存储，如`cache()`为StorageLevel为StorageLevel.MEMORY_ONLY的快捷方法。\n",
    "\n",
    "**Storage Level**|**Meaning**\n",
    ":--|:--\n",
    "MEMORY_ONLY | 默认级别。RDD以反序列化Java对象的形式存储在JVM中。若内存容纳不下RDD，则存储不下的分区不会被存储，需要用时重新计算。\n",
    "MEMORY_AND_DISK | RDD以反序列化Java对象的形式存储在JVM中。若内存容纳不下RDD，则存储不下的分区将存储到硬盘中，需要用时读取。\n",
    "MEMORY_ONLY_SER (Java and Scala) | RDD以序列化Java对象的形式存储在JVM中（每个分期一个字节数组）。相比反序列化Java对象，序列化Java对象占用内存更少，但代价是读取时需要更多的计算。\n",
    "MEMORY_AND_DISK_SER (Java and Scala) | 同MEMORY_AND_DISK，但存储的时序列化Java对象。\n",
    "DISK_ONLY | RDD分区都存储在硬盘上\n",
    "MEMORY_ONLY_2,<br>MEMORY_AND_DISK_2,etc. | 上述各级别的备份版本，会将每个分区备份到两个集群节点上\n",
    "OFF_HEAP | 类似MEMORY_ONLY_SER，但数据存在JVM的堆外内存（off-heap memory）。\n",
    "\n",
    "**注意**: Python中不管选择哪个级别，存储对象都会通过[Pickle](https://docs.python.org/2/library/pickle.html)进行序列化。\n",
    "\n",
    "Spark的部分重组操作（如：reduceByKey）中，也会自动持久化一些中间数据，这样做是为防止重组失败时重新从头开始计算。\n",
    "\n",
    "StorageLevel旨在提供不同内存占用和CPU效率之间的权衡点。如果需要计算快，使用默认的MEMORY_ONLY即可；若需要较小内存和较快计算，选择序列化存储方式；尽量不要选择存储到磁盘上的选项，除非计算RDD的代价远比在磁盘上读取RDD的代价大。若需要快速的故障恢复，可采用备份版本的持久化。\n",
    "\n",
    "Spark会自动监视每个节点上的缓存数据的使用，并根据最近最少使用算法（LRU，least-recently-used）删除旧的数据分区。当然也可通过`RDD.unpersist()`方法移除数据，注意该方法默认不阻塞，若要阻塞至资源释放，需指定`blocking=true`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共享变量\n",
    "\n",
    "Spark中，一般传至远程节点中的执行函数中用到的变量都是副本，其修改并不会改变驱动程序中的变量。在任务间支持通用读写共享变量是低效的，但为了通用化，Spark中还是提供了两种共享变量方式：广播变量（broadcast variables）和累加器（accumulators）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 广播变量\n",
    "\n",
    "广播变量允许程序员在每台机器上缓存一个只读变量，而不是随任务一起发送它的副本。Spark也尝试高效的广播算法来散播广播变量，以减少交流代价。\n",
    "\n",
    "Spark的重组操作将动作执行划分为一系列阶段，每个阶段中Spark会自动广播任务所需的通用数据。广播数据以序列化形式缓存，在执行计算任务之前反序列化。这意味着，仅当跨多阶段的任务需要相同数据或以非序列化形式缓存数据很重要时，显式创建广播变量才会显得有用。\n",
    "\n",
    "将变量v创建为广播变量：`SparkContxt.broadcast(v)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar = sc.broadcast([1, 2, 3])\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在创建broadcastVar变量之后，应该在集群上运行的任何函数中使用broadcastVar变量而不是值v，这样v就不会被多次发送到节点。此外，对象v在广播之后不应该被修改，以确保所有节点获得广播变量的相同值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "释放拷贝至执行程序的广播变量：`.unpersist()`，若后续再用则会重新广播\n",
    "\n",
    "释放该广播变量的所以资源：`.destroy()`，后续无法再用\n",
    "\n",
    "以上方法同都是默认不阻塞的，若要阻塞至资源释放，需指定`blocking=true`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 累加器\n",
    "\n",
    "累加器是满足结合律和交换律的算子经“累加”得到的变量，这些运算可以并行计算以提高效率。\n",
    "\n",
    "创建初始值为v的累加器：`SparkContext.accumulator(v)`，此后运行在同一集群上的任务可通过`add`或`+=`进行累加操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "accum\n",
    "\n",
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))\n",
    "\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前Spark原生支持数值型(numeric)累加器，用户可自定义新的类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "def g(x):\n",
    "    accum.add(x)\n",
    "    return x\n",
    "rdd = sc.parallelize([1, 2, 3, 4]).map(g)\n",
    "\n",
    "accum.value\n",
    "# 此处accum为0，因为`map`操作在此处还没有执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()\n",
    "# 执行动作collect后，map操作也会进行计算\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "202.386px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
